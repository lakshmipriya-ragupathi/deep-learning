{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplement the feedforward and backpropagation learning algorithm for multiple perceptrons in Python for the question provided in the attached image.\\n\\nInitialize the weights and biases randomly.\\n\\nImplement the forward pass.\\n\\nCompute the loss between the predicted output \\nand the actual output using an appropriate loss function.\\n\\nCompute the gradients of the loss function with respect to the weights \\nand biases using the chain rule.\\n\\nUpdate the weights and biases.\\n\\nIterate over multiple times (epochs), performing forward propagation, \\nloss calculation, backpropagation, and parameter updates in each iteration till convergence.\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Implement the feedforward and backpropagation learning algorithm for multiple perceptrons in Python for the question provided in the attached image.\n",
    "\n",
    "Initialize the weights and biases randomly.\n",
    "\n",
    "Implement the forward pass.\n",
    "\n",
    "Compute the loss between the predicted output \n",
    "and the actual output using an appropriate loss function.\n",
    "\n",
    "Compute the gradients of the loss function with respect to the weights \n",
    "and biases using the chain rule.\n",
    "\n",
    "Update the weights and biases.\n",
    "\n",
    "Iterate over multiple times (epochs), performing forward propagation, \n",
    "loss calculation, backpropagation, and parameter updates in each iteration till convergence.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.5,  1.5,  0.8],\n",
       "        [ 0.8,  0.2, -1.6]]),\n",
       " array([[ 0.9, -1.7,  1.6],\n",
       "        [ 1.2,  2.1, -0.2]]),\n",
       " array([1. , 0.7, 1.2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first im going to perform one iteration with the weight values given in class to check the correctness of the code: \n",
    "\n",
    "w1 = np.array([[0.5, 1.5, 0.8],[0.8,0.2,-1.6]])\n",
    "w2 = np.array([[0.9, -1.7, 1.6],[1.2,2.1,-0.2]])\n",
    "x = np.array([1, 0.7, 1.2])\n",
    "target = np.array([1, 0])\n",
    "w1, w2, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the sigmoid activation function \n",
    "def activation_function(num):\n",
    "    return 1 / (1 + (math.e ** (-num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of nodes FORMAT : O_nodePosition_layerValue, e.g. O_0_1 means Output of 0th node in 1st layer\n",
      "\n",
      "Output of nodes in layer 1\n",
      "O_0_1:1.0\tO_1_1:0.9248398905178734\tO_2_1:0.2728917836588705\n",
      "Output of nodes in layer 2\n",
      "O_0_2:0.441370708075126 \tO_1_2:0.9563777409741487\n"
     ]
    }
   ],
   "source": [
    "#implement forward pass\n",
    "def forward_pass(w1, w2, x):\n",
    "    layer_0_layer_1 = np.dot(x, w1.T)  \n",
    "    #print(f\"** x:{x} , w1:{w1}, x_shape:{x.shape}, w1_shape:{w1.shape}, mul:{layer_0_layer_1}\")\n",
    "    #print(f\"mul:{layer_0_layer_1}\")\n",
    "    layer_1_output = [activation_function(val) for val in layer_0_layer_1]\n",
    "    layer_1_output = np.insert(layer_1_output, 0, 1)\n",
    "    layer_1_layer_2 = np.dot(layer_1_output , w2.T)\n",
    "    layer_2_output = [activation_function(val) for val in layer_1_layer_2]\n",
    "    return layer_2_output, layer_1_output\n",
    "\n",
    "output_2, output_1 = forward_pass(w1, w2, x)\n",
    "print(f\"Output of nodes FORMAT : O_nodePosition_layerValue, e.g. O_0_1 means Output of 0th node in 1st layer\\n\")\n",
    "print(f\"Output of nodes in layer 1\\nO_0_1:{output_1[0]}\\tO_1_1:{output_1[1]}\\tO_2_1:{output_1[2]}\")\n",
    "print(f\"Output of nodes in layer 2\\nO_0_2:{output_2[0]} \\tO_1_2:{output_2[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function computes derivative of error at the output layer for all nodes\n",
    "def compute_error_output_layer(output, target):\n",
    "    error_at_layer = np.array([])\n",
    "    for i in range(0,len(output)):\n",
    "        val = (output[i] - target[i]) * output[i] * (1 - output[i])\n",
    "        error_at_layer = np.append(error_at_layer, val)\n",
    "    return error_at_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 after first epoch : [[ 0.48928025  1.49249617  0.7871363 ]\n",
      " [ 0.8229341   0.21605387 -1.57247908]]\n",
      "W2 after first epoch : [[ 0.96886855 -1.63630762  1.61879366]\n",
      " [ 1.18005027  2.08154969 -0.20544412]]\n"
     ]
    }
   ],
   "source": [
    "def backprop(w1,w2,x,output_2, output_1, target):\n",
    "    learning_rate = 0.5\n",
    "\n",
    "    error_at_2 = compute_error_output_layer(output_2, target)\n",
    "\n",
    "    #computing derivatives of weights with respect to error between layer 2 and layer 1\n",
    "    #derivatives with respect to X_0_1 , X_1_1, X_2_1\n",
    "\n",
    "    #defining a matrix of size (2,3) to store all the derivatives for each weight \n",
    "    dE_dw_2_1 = np.zeros_like(w2)\n",
    "    for i in range(len(error_at_2)):\n",
    "        for j in range(len(output_1)):\n",
    "            dE_dw_2_1[i][j] = error_at_2[i] * output_1[j]\n",
    "\n",
    "    # UPDATING WEIGHTS BETWEEN LAYER 2 AND LAYER 1\n",
    "    for i in range(len(w2)):\n",
    "        for j in range(len(w2[0])):\n",
    "            w2[i][j] -= learning_rate * dE_dw_2_1[i][j]\n",
    "\n",
    "    \n",
    "    #computing derivatives of weights with respect to error between layer 1 and layer 0\n",
    "            \n",
    "    #derivative of error at layer 1:  ,basically backpropagating error from layer 2 to layer 1 \n",
    "    error_node_1 = (error_at_2[0] * w2[0][1] + error_at_2[1] * w2[1][1]) * output_1[1] * (1 - output_1[1])\n",
    "    error_node_2 = (error_at_2[0] * w2[0][2] + error_at_2[1] * w2[1][2]) * output_1[2] * (1 - output_1[2])\n",
    "\n",
    "    error_at_1 = np.array([error_node_1, error_node_2])\n",
    "\n",
    "    #derivative of error with respect to weights between nodes in layer 1 and layer 0\n",
    "    dE_dw_1_0 = np.zeros_like(w1)\n",
    "    for i in range(len(error_at_1)):\n",
    "        for j in range(len(x)):\n",
    "            dE_dw_1_0[i][j] = error_at_1[i] * x[j]\n",
    "\n",
    "    #updating the weights between layer 0 and layer 1\n",
    "    w1 -= learning_rate * dE_dw_1_0\n",
    "\n",
    "    return w2, w1\n",
    "\n",
    "w2_new, w1_new = backprop(w1,w2,x,output_2,output_1,target)\n",
    "print(f\"W1 after first epoch : {w1_new}\")\n",
    "print(f\"W2 after first epoch : {w2_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_error_function(output, target):\n",
    "    error = 0.0\n",
    "    for i in range(len(output)):\n",
    "        error += ((output[i] - target[i]) ** 2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(x, target, w1, w2, epochs, tolerance):\n",
    "    number_of_epochs_needed = 0\n",
    "    for epoch in range(epochs):\n",
    "        number_of_epochs_needed += 1\n",
    "        output_2, output_1 = forward_pass(w1, w2, x)\n",
    "        if mse_error_function(output_2, target) < tolerance:\n",
    "            break\n",
    "        else:\n",
    "            w2, w1 = backprop(w1,w2,x,output_2, output_1, target)\n",
    "    return w1,w2, number_of_epochs_needed, output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 optimised : [[ 0.08929562  1.21250694  0.30715475]\n",
      " [ 1.8756602   0.95296214 -0.30920775]]\n",
      "W2 optimised : [[ 2.34102724 -0.46644573  2.43300646]\n",
      " [-1.84235253 -0.45820664 -2.11974796]]\n",
      "number of epochs : 1573\n",
      "Actual Output : [0.9846022106089666, 0.016209925753329742]\n",
      "Target Output : [1 0]\n",
      "MSE : 0.0004998536110590842\n"
     ]
    }
   ],
   "source": [
    "#lets find the optimised weights and bias with starting weight as the ones taught in class\n",
    "w1 = np.array([[0.5, 1.5, 0.8],[0.8,0.2,-1.6]])\n",
    "w2 = np.array([[0.9, -1.7, 1.6],[1.2,2.1,-0.2]])\n",
    "x = np.array([1, 0.7, 1.2])\n",
    "target = np.array([1, 0])\n",
    "epochs = 2000\n",
    "tolerance = 0.0005\n",
    "final_w1, final_w2, number_of_epochs, actual_output = training_loop(x, target, w1, w2, epochs, tolerance)\n",
    "\n",
    "print(f\"W1 optimised : {final_w1}\")\n",
    "print(f\"W2 optimised : {final_w2}\")\n",
    "print(f\"number of epochs : {number_of_epochs}\")\n",
    "print(f\"Actual Output : {actual_output}\")\n",
    "print(f\"Target Output : {target}\")\n",
    "print(f\"MSE : {mse_error_function(actual_output, target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 optimised : [[ 1.13906699  2.86295556  2.21451778]\n",
      " [ 1.36599669  0.16707079 -0.04799095]]\n",
      "W2 optimised : [[ 0.2682638   2.69225504  1.87995681]\n",
      " [-1.01868421 -3.07137003  0.18702629]]\n",
      "number of epochs : 1205\n",
      "Actual Output : [0.988659557208424, 0.019264997579689906]\n",
      "Target Output : [1 0]\n",
      "MSE : 0.0004997457744544655\n"
     ]
    }
   ],
   "source": [
    "#now lets randomly choose weights and train \n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "W1 = np.random.uniform(0,3, size = (2,3))\n",
    "W2 = np.random.uniform(0,3, size=(2,3))\n",
    "X = np.array([1, 0.7, 1.2])\n",
    "Target = np.array([1, 0])\n",
    "epochs = 2000\n",
    "tolerance = 0.0005\n",
    "\n",
    "final_w1, final_w2, number_of_epochs, actual_output = training_loop(X, Target, W1, W2, epochs, tolerance)\n",
    "\n",
    "print(f\"W1 optimised : {final_w1}\")\n",
    "print(f\"W2 optimised : {final_w2}\")\n",
    "print(f\"number of epochs : {number_of_epochs}\")\n",
    "print(f\"Actual Output : {actual_output}\")\n",
    "print(f\"Target Output : {Target}\")\n",
    "print(f\"MSE : {mse_error_function(actual_output, Target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are multiple W1 and W2 matrices that could be initiated which would give the same target values \n",
    "Based on the tolerance 0.0005, the teacher given weights and randomly selected weights usually converge around 1500 iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
